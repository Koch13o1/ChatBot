Document Embedding and Query System
This project leverages Sentence Transformers and Chroma to embed documents and create a searchable database for efficient question answering. It consists of two main parts:

Document Embedding and Database Creation (create_database.py): Embeds documents into vector representations and stores them in a Chroma database.
Query Processing and Answering (query_data.py): Allows users to query the embedded database for answers based on relevant context using OpenAI models.
Requirements
Python 3.8 or later
Libraries:
sentence-transformers
langchain-community
langchain-openai
dotenv
argparse
shutil
You can install all dependencies via pip:

```
pip install -r requirements.txt
```

Project Structure
create_database.py: This script loads documents (Markdown files), splits them into chunks, generates embeddings for each chunk, and stores them in a Chroma vector database.
query_data.py: This script allows querying the Chroma vector database. It returns relevant chunks of text based on the user’s query, then uses an OpenAI model to generate a response using those chunks as context.
Example Directory Structure:
```
.
├── data
│   └── books
│       ├── book1.md
│       └── book2.md
├── chroma
├── create_database.py
├── query_data.py
├── requirements.txt
└── .env
```
Usage
Step 1: Create the Database (Embedding)
Run the create_database.py script to create the database by embedding the documents:

```
python create_database.py
```
This will:

Load all Markdown files in the data/books directory.
Split the documents into chunks.
Generate embeddings for each chunk.
Save the embeddings in a Chroma database located in the chroma directory.
Step 2: Query the Database
After the database is created, you can query it using query_data.py. Provide a query string, and the script will search for relevant chunks of text in the database and return a response generated by an OpenAI model:

```
python query_data.py "What is the main theme of the book?"
```
This will:

Search for the top 3 most relevant chunks in the database based on the query.
Use OpenAI to generate a response based on those chunks.
Print the response along with the sources from where the information was derived.
Example Query and Output:
```
python query_data.py "What is the main theme of the book?"


Answer the question based only on the following context:

The main theme of the book is about personal growth and discovery...

---

Answer the question based on the above context: What is the main theme of the book?

Response: The main theme of the book revolves around personal growth and discovery.
Sources: ['book1.md', 'book2.md']
```
Notes
Data Format: The project expects Markdown files (.md) in the data/books directory. You can modify the DATA_PATH variable to point to any other directory if needed.
Embedding Model: The script uses the Hugging Face model all-MiniLM-L6-v2 for embedding documents, but you can change it to any other suitable model.
Chroma: The Chroma database is used to store and query document embeddings. The database is saved in the chroma directory by default.
